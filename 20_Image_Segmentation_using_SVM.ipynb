{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    " \n",
    "#img = cv2.imread('BSE_Image.jpg')\n",
    "img = cv2.imread('images/Train_images/Sandstone_Versa0000.tif')\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  \n",
    "#Here, if you have multichannel image then extract the right channel instead of converting the image to grey. \n",
    "#For example, if DAPI contains nuclei information, extract the DAPI channel image first. \n",
    "\n",
    "#Multiple images can be used for training. For that, you need to concatenate the data\n",
    "\n",
    "#Save original image pixels into a data frame. This is our Feature #1.\n",
    "img2 = img.reshape(-1)\n",
    "df = pd.DataFrame()\n",
    "df['Original Image'] = img2\n",
    "\n",
    "#Generate Gabor features\n",
    "num = 1  #To count numbers up in order to give Gabor features a lable in the data frame\n",
    "kernels = []\n",
    "for theta in range(2):   #Define number of thetas\n",
    "    theta = theta / 4. * np.pi\n",
    "    for sigma in (1, 3):  #Sigma with 1 and 3\n",
    "        for lamda in np.arange(0, np.pi, np.pi / 4):   #Range of wavelengths\n",
    "            for gamma in (0.05, 0.5):   #Gamma values of 0.05 and 0.5\n",
    "            \n",
    "                \n",
    "                gabor_label = 'Gabor' + str(num)  #Label Gabor columns as Gabor1, Gabor2, etc.\n",
    "#                print(gabor_label)\n",
    "                ksize=9\n",
    "                kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lamda, gamma, 0, ktype=cv2.CV_32F)    \n",
    "                kernels.append(kernel)\n",
    "                #Now filter the image and add values to a new column \n",
    "                fimg = cv2.filter2D(img2, cv2.CV_8UC3, kernel)\n",
    "                filtered_img = fimg.reshape(-1)\n",
    "                df[gabor_label] = filtered_img  #Labels columns as Gabor1, Gabor2, etc.\n",
    "                print(gabor_label, ': theta=', theta, ': sigma=', sigma, ': lamda=', lamda, ': gamma=', gamma)\n",
    "                num += 1  #Increment for gabor column label\n",
    "                \n",
    "########################################\n",
    "#Gerate OTHER FEATURES and add them to the data frame\n",
    "                \n",
    "#CANNY EDGE\n",
    "edges = cv2.Canny(img, 100,200)   #Image, min and max values\n",
    "edges1 = edges.reshape(-1)\n",
    "df['Canny Edge'] = edges1 #Add column to original dataframe\n",
    "\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "\n",
    "#ROBERTS EDGE\n",
    "edge_roberts = roberts(img)\n",
    "edge_roberts1 = edge_roberts.reshape(-1)\n",
    "df['Roberts'] = edge_roberts1\n",
    "\n",
    "#SOBEL\n",
    "edge_sobel = sobel(img)\n",
    "edge_sobel1 = edge_sobel.reshape(-1)\n",
    "df['Sobel'] = edge_sobel1\n",
    "\n",
    "#SCHARR\n",
    "edge_scharr = scharr(img)\n",
    "edge_scharr1 = edge_scharr.reshape(-1)\n",
    "df['Scharr'] = edge_scharr1\n",
    "\n",
    "#PREWITT\n",
    "edge_prewitt = prewitt(img)\n",
    "edge_prewitt1 = edge_prewitt.reshape(-1)\n",
    "df['Prewitt'] = edge_prewitt1\n",
    "\n",
    "#GAUSSIAN with sigma=3\n",
    "from scipy import ndimage as nd\n",
    "gaussian_img = nd.gaussian_filter(img, sigma=3)\n",
    "gaussian_img1 = gaussian_img.reshape(-1)\n",
    "df['Gaussian s3'] = gaussian_img1\n",
    "\n",
    "#GAUSSIAN with sigma=7\n",
    "gaussian_img2 = nd.gaussian_filter(img, sigma=7)\n",
    "gaussian_img3 = gaussian_img2.reshape(-1)\n",
    "df['Gaussian s7'] = gaussian_img3\n",
    "\n",
    "#MEDIAN with sigma=3\n",
    "median_img = nd.median_filter(img, size=3)\n",
    "median_img1 = median_img.reshape(-1)\n",
    "df['Median s3'] = median_img1\n",
    "\n",
    "#VARIANCE with size=3\n",
    "variance_img = nd.generic_filter(img, np.var, size=3)\n",
    "variance_img1 = variance_img.reshape(-1)\n",
    "df['Variance s3'] = variance_img1  #Add column to original dataframe\n",
    "\n",
    "\n",
    "######################################                \n",
    "\n",
    "#Now, add a column in the data frame for the Labels\n",
    "#For this, we need to import the labeled image\n",
    "labeled_img = cv2.imread('images/Train_masks/Sandstone_Versa0000.tif')\n",
    "#Remember that you can load an image with partial labels \n",
    "#But, drop the rows with unlabeled data\n",
    "\n",
    "labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_BGR2GRAY)\n",
    "labeled_img1 = labeled_img.reshape(-1)\n",
    "df['Labels'] = labeled_img1\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "#df.to_csv(\"Gabor.csv\")\n",
    "\n",
    "\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the dependent variable that needs to be predicted (labels)\n",
    "Y = df[\"Labels\"].values\n",
    "\n",
    "#Define the independent variables\n",
    "X = df.drop(labels = [\"Labels\"], axis=1) \n",
    "\n",
    "#Split data into train and test to verify accuracy after fitting the model. \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# # Instantiate model with n number of decision trees\n",
    "# model = RandomForestClassifier(n_estimators = 100, random_state = 42)\n",
    "\n",
    "\n",
    "###\n",
    "#SVM\n",
    "# Train the Linear SVM to compare against Random Forest\n",
    "#SVM will be slower than Random Forest. \n",
    "#Make sure to comment out Fetaure importances lines of code as it does not apply to SVM.\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(max_iter=1000)  #Default of 100 is not converging\n",
    "\n",
    "# Train the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# verify number of trees used. If not defined above. \n",
    "#print('Number of Trees used : ', model.n_estimators)\n",
    "\n",
    "#STEP 8: TESTING THE MODEL BY PREDICTING ON TEST DATA\n",
    "#AND CALCULATE THE ACCURACY SCORE\n",
    "#First test prediction on the training data itself. SHould be good. \n",
    "prediction_test_train = model.predict(X_train)\n",
    "\n",
    "#Test prediction on testing data. \n",
    "prediction_test = model.predict(X_test)\n",
    "\n",
    "#.predict just takes the .predict_proba output and changes everything \n",
    "#to 0 below a certain threshold (usually 0.5) respectively to 1 above that threshold.\n",
    "#In this example we have 4 labels, so the probabilities will for each label stored separately. \n",
    "# \n",
    "#prediction_prob_test = model.predict_proba(X_test)\n",
    "\n",
    "#Let us check the accuracy on test data\n",
    "from sklearn import metrics\n",
    "#Print the prediction accuracy\n",
    "\n",
    "#First check the accuracy on training data. This will be higher than test data prediction accuracy.\n",
    "print (\"Accuracy on training data = \", metrics.accuracy_score(y_train, prediction_test_train))\n",
    "#Check accuracy on test dataset. If this is too low compared to train it indicates overfitting on training data.\n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_test))\n",
    "\n",
    "\n",
    "\n",
    "#This part commented out for SVM testing. Uncomment for random forest. \n",
    "#One amazing feature of Random forest is that it provides us info on feature importances\n",
    "# Get numerical feature importances\n",
    "#importances = list(model.feature_importances_)\n",
    "\n",
    "#Let us print them into a nice format.\n",
    "\n",
    "feature_list = list(X.columns)\n",
    "feature_imp = pd.Series(model.feature_importances_,index=feature_list).sort_values(ascending=False)\n",
    "print(feature_imp)\n",
    "\n",
    "\n",
    "#You can store the model for future use. In fact, this is how you do machine elarning\n",
    "#Train on training images, validate on test images and deploy the model on unknown images. \n",
    "\n",
    "import pickle\n",
    "\n",
    "#Save the trained model as pickle string to disk for future use\n",
    "filename = \"sandstone_model\"\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "#To test the model on future datasets\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict(X)\n",
    "\n",
    "segmented = result.reshape((img.shape))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(segmented, cmap ='jet')\n",
    "plt.imsave('segmented_rock_RF_100_estim.jpg', segmented, cmap ='jet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
